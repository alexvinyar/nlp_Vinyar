{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Корпус\n",
    "1000 предложений со словом \"человек\" (0) и 1000 предложений со словом \"пароход\" (1) из основного корпуса НКРЯ. Заменены на Xx_xX\n",
    "\n",
    "## Предобработка\n",
    "Токенизация - word_tokenize из NLTK, лемматизация - pymorphy2, стоп-слова - из NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "orig_sents = []\n",
    "sents = []\n",
    "classes = []\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stopw = set(stopwords.words('russian'))\n",
    "punct = string.punctuation + '`―«»'\n",
    "with open('man-ship.txt','r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.strip():\n",
    "            cls = line.strip().split('\\t')[0]\n",
    "            sent = line.strip().split('\\t')[1]\n",
    "            orig_sents.append(sent)\n",
    "            tokens = word_tokenize(sent)\n",
    "            lemmas = [morph.parse(x)[0].normal_form for x in tokens if x.lower() not in stopw and x.strip(punct)]\n",
    "            sents.append(' '.join(lemmas))\n",
    "            classes.append(int(cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация\n",
    "Два типа - CountVectorizer и TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "count_sents = count_vect.fit_transform(sents)\n",
    "tfidf_sents = tfidf_vect.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_count_train, X_count_test, y_count_train, y_count_test = train_test_split(count_sents, classes, test_size=0.3, random_state=42)\n",
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(tfidf_sents, classes, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификаторы\n",
    "* Naive Bayes\n",
    "* Logistic Regression\n",
    "* SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + NaiveBayes: 0.879338842975\n",
      "TfIdfVectorizer + NaiveBayes: 0.891891891892\n"
     ]
    }
   ],
   "source": [
    "bayes = MultinomialNB()\n",
    "bayes.fit(X_count_train,y_count_train)\n",
    "print('CountVectorizer + NaiveBayes:',f1_score(y_count_test,bayes.predict(X_count_test)))\n",
    "\n",
    "bayes.fit(X_tfidf_train,y_tfidf_train)\n",
    "print('TfIdfVectorizer + NaiveBayes:',f1_score(y_tfidf_test,bayes.predict(X_tfidf_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + LogisticRegression: 0.863945578231\n",
      "TfIdfVectorizer + LogisticRegression: 0.867671691792\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(random_state=42)\n",
    "logistic.fit(X_count_train,y_count_train)\n",
    "print('CountVectorizer + LogisticRegression:',f1_score(y_count_test,logistic.predict(X_count_test)))\n",
    "\n",
    "logistic.fit(X_tfidf_train,y_tfidf_train)\n",
    "print('TfIdfVectorizer + LogisticRegression:',f1_score(y_tfidf_test,logistic.predict(X_tfidf_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + SVM: 0.849829351536\n",
      "TfIdfVectorizer + SVM: 0.877721943049\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_count_train,y_count_train)\n",
    "print('CountVectorizer + SVM:',f1_score(y_count_test,svm.predict(X_count_test)))\n",
    "\n",
    "svm.fit(X_tfidf_train,y_tfidf_train)\n",
    "print('TfIdfVectorizer + SVM:',f1_score(y_tfidf_test,svm.predict(X_tfidf_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       | NaiveBayes     | LogisticRegression | LinearSVC      |\n",
    "|-------|----------------|--------------------|----------------|\n",
    "| Count | 0.879338842975 | 0.863945578231     | 0.849829351536 |\n",
    "| TfIdf | 0.891891891892 | 0.867671691792     | 0.877721943049 |\n",
    "\n",
    "Лучше всего работает наивный байесовский классификатор с tf-idf векторизатором.\n",
    "\n",
    "## Уменьшение размерности\n",
    "Минимальная частота слова - 5, максимальная - 500. Тестируем на логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + LogisticRegression: 0.843333333333\n",
      "TfIdfVectorizer + LogisticRegression: 0.852404643449\n"
     ]
    }
   ],
   "source": [
    "small_count_vect = CountVectorizer(min_df=5,max_df=500)\n",
    "small_tfidf_vect = TfidfVectorizer(min_df=5,max_df=500)\n",
    "\n",
    "count_sents = small_count_vect.fit_transform(sents)\n",
    "tfidf_sents = small_tfidf_vect.fit_transform(sents)\n",
    "\n",
    "X_count_train, X_count_test, y_count_train, y_count_test = train_test_split(count_sents, classes, test_size=0.3, random_state=42)\n",
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(tfidf_sents, classes, test_size=0.3, random_state=42)\n",
    "\n",
    "logistic = LogisticRegression(random_state=42)\n",
    "logistic.fit(X_count_train,y_count_train)\n",
    "print('CountVectorizer + LogisticRegression:',f1_score(y_count_test,logistic.predict(X_count_test)))\n",
    "\n",
    "logistic.fit(X_tfidf_train,y_tfidf_train)\n",
    "print('TfIdfVectorizer + LogisticRegression:',f1_score(y_tfidf_test,logistic.predict(X_tfidf_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает хуже.\n",
    "\n",
    "## Ошибки  (NaiveBayes + TfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pred sent\n",
      "1 0  Хотя бы ради того, чтобы прикоснуться к советской жизни, на самый короткий срок, быстрым Xx_xX, без всяких задержек в Европе, получить ваши указания, урегулировать ряд практических дел…».\n",
      "0 1  А он, деликатный Xx_xX, попросил, чтобы охрана поехала сзади.\n",
      "1 0  Процесс производства, например, импортного масла «Анкор» выглядит так: 25-килограммовые блоки масла Xx_xX приплывают из Новой Зеландии, размягчаются и фасуются в стандартные пачки.\n",
      "1 0  ― Как это так, ведь Xx_xX больше не приходил.\n",
      "0 1  Xx_xX, передавший бумаги, рассказал, что служебных записок от Дурманова было несколько ― написанных в разное время, но на одну тему.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "cls_pred = bayes.predict(tfidf_sents)\n",
    "errors = random.sample([(i,classes[i],x) for i,x in enumerate(cls_pred) if x != classes[i]],5)\n",
    "print('true','pred','sent')\n",
    "for e in errors:\n",
    "    print(e[1],e[2],orig_sents[e[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибки (кроме четвертой, пожалуй) странные, не очень понятно, какие слова их вызывают.\n",
    "\n",
    "## Примеры работы (NaiveBayes + TfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sents_man = [\n",
    "    'И сама жизнь Xx_xX ― это необратимый процесс.',\n",
    "    'Также ясно вижу и масштаб проблем, которые были бы связаны с любым Xx_xX, который имел шансы стать президентом России вместо Путина.',\n",
    "    'Раньше считалось, что в России средний Xx_xX может прожить на 1137 руб. в месяц.',\n",
    "    'Ты ведь взрослый Xx_xX и этого парнечка знаешь…',\n",
    "    'В колхозной конторе окна не светили, но на гул машин вышли два Xx_xX: взрослый и мальчишка.'\n",
    "]\n",
    "\n",
    "new_sents_ship = [\n",
    "    'Как только Xx_xX подваливал к пристани, они начинали оплакивать умершего высокими, томительными голосами.',\n",
    "    'Прилепившись присосками к отвесной скале, над затонувшим Xx_xX висел плоский диск.',\n",
    "    'Несколько взмахов весел донесли нас к борту югославского Xx_xX.',\n",
    "    'Xx_xX опоздал на шесть часов и только утром ушел из Казани.',\n",
    "    ' Нос отделился от кормы, и обе части Xx_xX, снятые с камней экспедицией Эпрона, стояли рядом, покачиваясь на якорях…'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pred sent\n",
      "man\n",
      "0 0 И сама жизнь Xx_xX ― это необратимый процесс.\n",
      "0 0 Также ясно вижу и масштаб проблем, которые были бы связаны с любым Xx_xX, который имел шансы стать президентом России вместо Путина.\n",
      "0 0 Раньше считалось, что в России средний Xx_xX может прожить на 1137 руб. в месяц.\n",
      "0 0 Ты ведь взрослый Xx_xX и этого парнечка знаешь…\n",
      "0 1 В колхозной конторе окна не светили, но на гул машин вышли два Xx_xX: взрослый и мальчишка.\n",
      "\n",
      "ship\n",
      "1 1 Как только Xx_xX подваливал к пристани, они начинали оплакивать умершего высокими, томительными голосами.\n",
      "1 1 Прилепившись присосками к отвесной скале, над затонувшим Xx_xX висел плоский диск.\n",
      "1 1 Несколько взмахов весел донесли нас к борту югославского Xx_xX.\n",
      "1 1 Xx_xX опоздал на шесть часов и только утром ушел из Казани.\n",
      "1 1  Нос отделился от кормы, и обе части Xx_xX, снятые с камней экспедицией Эпрона, стояли рядом, покачиваясь на якорях…\n"
     ]
    }
   ],
   "source": [
    "print('true','pred','sent')\n",
    "print('man')\n",
    "for sent in new_sents_man:\n",
    "    tokens = word_tokenize(sent)\n",
    "    lemmas = [morph.parse(x)[0].normal_form for x in tokens if x.lower() not in stopw and x.strip(punct)]\n",
    "    vect = tfidf_vect.transform([' '.join(lemmas)])\n",
    "    pred = bayes.predict(vect)\n",
    "    print(0,pred[0],sent)\n",
    "    \n",
    "print('\\nship')\n",
    "for sent in new_sents_ship:\n",
    "    tokens = word_tokenize(sent)\n",
    "    lemmas = [morph.parse(x)[0].normal_form for x in tokens if x.lower() not in stopw and x.strip(punct)]\n",
    "    vect = tfidf_vect.transform([' '.join(lemmas)])\n",
    "    pred = bayes.predict(vect)\n",
    "    print(1,pred[0],sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна ошибка в примерах на \"человек\", вероятно, из-за слова \"гул\" и, может быть, \"машин\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
